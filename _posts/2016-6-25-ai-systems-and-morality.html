---
layout: post
title: Rise of the AI Systems and Morality
---

<p>
It is fact that Artificially Intelligent systems are on rise. As we speak, researchers are trying hard to make them smarter. But intelligence is not the only characteristic that human brain possesses. The ability of a human being to think with conscience is something that separates these learning algorithms from human brain. Do these systems need to learn morals? Moreover, is it possible to embed this ethical thinking into AI systems programmatically?
</p>
<!--excerpt-->

<div class="row" style="padding:20px">
<img class="img-responsive" style="width:100%" src="/assets/img/blog/2016-6-25-google-car.jpg">
<p class="text-center" style="font-size:10pt">Source: https://commons.wikimedia.org/wiki/File:Google_self_driving_car_at_the_Googleplex.jpg</p>
</div>

<p>
<a href="http://arxiv.org/pdf/1510.03346v1.pdf" target="_blank">This paper describing the moral decision making ability necessary for an autonomous car</a> speaks about a scenario which shows that AI system should have conscience. Suppose you are sitting in such a car which is about to meet an unavoidable accident that might kill many pedestrians. Car faces two options. First, kill the pedestrians. Second, swirl around, and dash a wall besides the road. What option would you choose? It seems surveys shows that people want car to kill its owner. This gives rise to many like philosophical questions such as, why should owner pay for misbehaving pedestrians. Who should be held guilty for such incidents? Should the decision be based on who is involved in the accident (eg, children)? And if cars are programmed to kill their owner, would you buy such cars?
</p><br>

<p>
But this doesn't stop here. Military currently does not allow any fully automated defense system. But at the rate we are moving ahead, a day would come when this would become a reality. In a warfare, if a particular autonomous system does not follow certain international war ethics, due to say technical glitch, who is to be held resposible?
</p><br>

<p>
This was about the AI systems which generally interact with environment. But there are other ethical issues that we have no solution about. Remember <a href="http://www.businessinsider.in/Microsoft-is-deleting-its-AI-chatbots-incredibly-racist-tweets/articleshow/51539858.cms" target="_blank">Microsoft's Tay fiasco on twitter</a>? The self learning twitter bot from Microsoft started learning racism, abuses and trolls that lead to the huge embarassment to Microsoft. The bot was then hastily put down.
</p><br>

<p>
According to <a href="http://thehumanist.com/magazine/july-august-2014/up-front/robo-morality" target="_blank">this article</a>, Office of Naval Research (ONR) has granted of $7.5 million to university researchers at Tufts, Rensselaer Polytechnic Institute, Brown, Yale, and Georgetown to build ethical decision-making. It would be nice if this module works in “plug and play fashion” which inhibits the actions of systems if required.
</p><br>

<p>s
Offcourse, this could lead to adverse side effects. What if robot becomes too emotional? Can this play as security loophole? <a href="https://www.quora.com/Is-it-ethical-to-kill-a-robot-programmed-with-emotions" target="_blank">Is it ethical to kill a robot who knows emotions?</a>
</p><br>

<p>
This is now an open field with clean board. It would be interesting to see how research would take this forward.
</p>
