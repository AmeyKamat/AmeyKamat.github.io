---
layout: post
title: All You Need to Know About Wide and Deep Learning
---

<p>
Ranking is the heart of any recommender system. It is this module that dictates success of the system generally. Google in this week announced its decision to open source its ranking algorithm called "Wide and Deep Learning" for recommender systems in its research blog post. This system is available for use though the popular TensorFlow system. In this post, we would talk all about this new algorithm that Google gave away.
</p>
<!--excerpt-->

<div class="row text-center" style="padding:20px">
	<iframe width="853" height="480" src="https://www.youtube.com/embed/Xmw9SWJ0L50?rel=0" frameborder="0" allowfullscreen></iframe>
</div>

<p>
To give you an idea, recommendation systems generally take in query from the users as an input. A list of finite items are fetched from the database which match with highest degree with the query. Next, the items are ranked using a model which is already trained by a learner which uses historical logs to train the model. The generated ranking is then served to the user, and based on the users actions, the logs are updated.
</p><br>

<p>
The possible candidates for ranking systems are one, linear models and two, deep learning networks. Linear models (aka Wide Learning) are efficient in fitting the data which it has already seen demonstrating memorisation feature of the ranking system. But when it comes to predicting ranking based on unknown data, it performs poorly. On the other hand, the deep learniing networks (aka Deep Learning). <a href="http://www.ameykamat.in/blog/2016/06/17/turing-test-for-sound/" target="_blank">We have already covered how these networks have been used at MIT to artificially mimic sounds.</a> These kinds of models work well for generalised cases, but gives up for targetted queries.
</p><br>

<p>
The researchers at Google combined these two models with weighted sum, and fed it to the logistic loss error. The features are fed to the Wide component in raw form and transformed form (e.g., cross-product transformation). The features generated by word embedding using a separately trained neural net are fed to the Deep component. Word Embedding is a sophisticated technique that maps words to a specfic embedding vector. <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/" target="_blank">This blog article talks more about Word Embedding.</a> Both components are jointly trained using back-propogation, thus preparing system for the practical use.
</p><br>

<p>
The system was tested on 1% user base of Google Play which showed 3.9% improvement in app acquisition. Readers can access the <a href="http://arxiv.org/pdf/1606.07792v1.pdf" target="_blank">related research paper here</a>.
</p>
